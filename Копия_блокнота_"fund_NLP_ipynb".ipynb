{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"fund_NLP.ipynb\"",
      "provenance": [],
      "collapsed_sections": [
        "__FVPUrWevLQ",
        "xtr_Vk9UlQR3",
        "xQpD7fZO5Rtj",
        "URoVGM586Lcd",
        "PSuiPxZN9cJS",
        "quMSvPl798L5",
        "2jtcoxlAoI_J",
        "J7Xp2H-KC6Al"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Syava29/PracticeApp/blob/master/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22fund_NLP_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o0i_5KfLpy8"
      },
      "source": [
        "#from IPython.display import Image\n",
        "#from IPython.core.display import HTML\n",
        "#Image(filename = \"task1.png\", width=100, height=100) \n",
        "#Image(url= \"http://my_site.com/my_picture.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_nkjoB_7S2_"
      },
      "source": [
        "# 1. Теоретический материал по курсу"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5kbrw0P_LwU"
      },
      "source": [
        "## 1.1 Основные свойства текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYdF5OYS_L7b"
      },
      "source": [
        "## 1.2. Уровни рассмотрения текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcwc38Q2_ME7"
      },
      "source": [
        "## 1.3. Графематический анализ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ZtTFmG_MQx"
      },
      "source": [
        "## 1.4. Морфологический анализ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHC1YKcF_lQt"
      },
      "source": [
        "## 1.5. Синтаксический анализ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhtrBB9t_p3G"
      },
      "source": [
        "## 1.6. Семантический анализ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcFz2n59_MZW"
      },
      "source": [
        "## 1.7. Выделение словосочетаний"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu0IGkbC_lba"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbq18sb97a4u"
      },
      "source": [
        "## 2. Методы лингвистического анализа текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwE8WOXH7bDk"
      },
      "source": [
        "## 3. Методы векторного представления текстов  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHSM6fzk7bNu"
      },
      "source": [
        "## 4. Методы классификации "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_zoldZ77bc6"
      },
      "source": [
        "## 5. Методы кластерного анализа "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7COZZ5f28wQV"
      },
      "source": [
        "## 6. Отдельные задачи анализа текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnbNIMGKKx7M"
      },
      "source": [
        "# Задание на практические и лабораторные занятия:\n",
        "## <u>1. [Веб-скрейпинг](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%B1-%D1%81%D0%BA%D1%80%D0%B5%D0%B9%D0%BF%D0%B8%D0%BD%D0%B3#:~:text=%D0%92%D0%B5%D0%B1%2D%D1%81%D0%BA%D1%80%D0%B5%D0%B9%D0%BF%D0%B8%D0%BD%D0%B3%20(%D0%B8%D0%BB%D0%B8%20%D1%81%D0%BA%D1%80%D0%B5%D0%BF%D0%B8%D0%BD%D0%B3%2C,%D0%B8%D1%85%20%D1%81%D0%BE%20%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B8%D1%86%20%D0%B2%D0%B5%D0%B1%2D%D1%80%D0%B5%D1%81%D1%83%D1%80%D1%81%D0%BE%D0%B2.)</u>\n",
        "Задание: выбрать любую страницу ресурса [Электронная библиотека механико-математического факультета Московского государственного университета](http://lib.mexmat.ru/). Забрать следующую информацию:\n",
        "- название;\n",
        "- автор;\n",
        "- аннотация;\n",
        "- рубрика;\n",
        "- издание;\n",
        "- год издания.\n",
        "\n",
        "Требуется полученную информацию:\n",
        "\n",
        "1. <u>Вывести в формате</u>:\n",
        "\n",
        "`рубрика`\n",
        "\n",
        "`автор` `название` `издание` `год издания`\n",
        "\n",
        "`аннотация`\n",
        "\n",
        "2. <u>Записать в датафрейм (каждая книга в новой строке)</u>.\n",
        "\n",
        "## <u>2. [Базовые методы обработки текстов библиотеки NLTK](#fund_nltk)</u>\n",
        "\n",
        "Ознакомиться с базовыми методами обработки текстов библиотеки `NLTK`. Смотрите ниже соответствующий раздел.\n",
        "\n",
        "## <u>3. [Булева модель информационного поиска](https://aspirantura.hse.ru/data/2015/06/06/1097414180/2015-06-04-frolov.pdf)</u>\n",
        "\n",
        "Задание: Сформировать несколько текстов. Для данных текстов получить наборы слов в нормальной форме. Построить общий словарь для документов. Для поискового запроса определить тексты, содержащие компоненты запроса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIIJgR7pWOK2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUEV2YgrvNjP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prUltdCQQej1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCB3lcvUQYvo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjiHCp7ociGe"
      },
      "source": [
        "- Токенизация по предложениям.\n",
        "- Токенизация по словам.\n",
        "- Лемматизация и стемминг текста.\n",
        "- Стоп-слова.\n",
        "- Регулярные выражения.\n",
        "- Мешок слов.\n",
        "- TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_q7btw1bUDP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufGu9COdbb5q"
      },
      "source": [
        "# Основные пакеты Python для обработки естественного языка   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__FVPUrWevLQ"
      },
      "source": [
        "## Пакет `NLTK`\n",
        "\n",
        "[NLTK](https://ru.wikipedia.org/wiki/Natural_Language_Toolkit#:~:text=%D0%91%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B0%20NLTK%2C%20%D0%B8%D0%BB%D0%B8%20NLTK%2C%20%E2%80%94,%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%B8%20%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.&text=NLTK%20%D1%8F%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%D1%81%D1%8F%20%D1%81%D0%B2%D0%BE%D0%B1%D0%BE%D0%B4%D0%BD%D1%8B%D0%BC%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D0%BC%20%D0%BE%D0%B1%D0%B5%D1%81%D0%BF%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC,%D0%9F%D1%80%D0%BE%D0%B5%D0%BA%D1%82%20%D0%B2%D0%BE%D0%B7%D0%B3%D0%BB%D0%B0%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%20%D0%A1%D1%82%D0%B8%D0%B2%D0%B5%D0%BD%20%D0%91%D1%91%D1%80%D0%B4.) (Natural Language Tool-Kit — пакет инструментов для обработки естественного языка) — ресурс для обработки естественного языка, написанный на Python экспертами в академических кругах. Первоначально создававшийся как инструмент для обучения обработке естественных языков, этот пакет содержит корпусы, лексические ресурсы, грамматики, алгоритмы обработки языков и предварительно обученные модели, позволяя программистам на Python быстро приступать к обработке текстовых данных на разных естественных языках.\n",
        "\n",
        "*Дополнительные ресурсы*:\n",
        "1. [Основы Natural Language Processing для текста](https://habr.com/ru/company/Voximplant/blog/446738/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtr_Vk9UlQR3"
      },
      "source": [
        "## Библиотека `Gensim`\n",
        "\n",
        "[Gensim](https://en.wikipedia.org/wiki/Gensim) --- надежная, эффективная и простая в использовании библиотека, главной целью которой является семантическое моделирование текста без учителя. \n",
        "\n",
        "Первоначально создававшаяся для поиска сходств в документах, в настоящее время эта библиотека предоставляет тематическое моделирование для методов латентно-семантического анализа и включает в себя другие библиотеки машинного обучения без учителя, такие как word2vec.\n",
        "\n",
        "*Дополнительные ресурсы:*\n",
        "\n",
        "1. [Gensim — Руководство для начинающих](https://webdevblog.ru/gensim-rukovodstvo-dlya-nachinajushhih/);\n",
        "2. [gensim – Topic Modelling in Python](https://github.com/RaRe-Technologies/gensim/#documentation)\n",
        "3. [gensim 3.8.3](https://pypi.org/project/gensim/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQpD7fZO5Rtj"
      },
      "source": [
        "## Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0aOsUtp4dZL"
      },
      "source": [
        "[Scikit-Learn](https://scikit-learn.org/stable/index.html) — расширение для библиотеки SciPy (Scientific Python), предоставляющее прикладной интерфейс (API) для обобщенного машинного обучения. \n",
        "\n",
        "Scikit-Learn сочетает высокую производительность с простотой использования методов анализа наборов данных малого и среднего размера. \n",
        "\n",
        "Это расширение, распространяемое с открытым исходным кодом и допускающее коммерческое использование, предоставляет единый интерфейс для многих моделей регрессии, классификации, кластеризации и уменьшения размерности, а также утилиты для перекрестной проверки и настройки гиперпараметров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URoVGM586Lcd"
      },
      "source": [
        "## Yellowbrick\n",
        "\n",
        "[Yellowbrick](https://www.scikit-yb.org/en/latest/) — комплект инструментов визуальной диагностики для анализа и интерпретации результатов машинного обучения. Дополняя ScikitLearn API, пакет Yellowbrick предоставляет простые и понятные визуальные средства для выбора признаков, моделирования и настройки гиперпараметров, управления процессом выбора моделей, наиболее эффективно описывающих текстовые данные."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSuiPxZN9cJS"
      },
      "source": [
        "## spaCy \n",
        "\n",
        "[spaCy](https://spacy.io/) реализует высококачественную обработку естественного языка, обертывая современные академические алгоритмы в простой и удобный API. В частности, spaCy позволяет выполнить предварительную обработку текста в подготовке к глубокому обучению и может использоваться для создания систем извлечения информации или анализа естественного языка на больших объемах текста."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quMSvPl798L5"
      },
      "source": [
        "## NetworkX\n",
        "\n",
        "[NetworkX](https://networkx.org/) — комплексный пакет для анализа графов, помогающий создавать, упорядочивать, анализировать сложные сетевые структуры и манипулировать ими. Несмотря на то что он не является библиотекой машинного обучения или анализа текстов, применение графовых структур данных позволяет кодировать сложные отношения, которые графовые алгоритмы способны анализировать и находить смысловые особенности, а следовательно, является важным инструментом анализа текста."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5KLpc7YA6bt"
      },
      "source": [
        "## TextBlob\n",
        "\n",
        "[TextBlob](https://textblob.readthedocs.io/en/dev/) — объектно-ориентированная библиотека NLP-обработки текста, построенная на базе NLP-библиотек NLTK и pattern и упрощающая некоторые аспекты их функциональности. Вот примеры операций NLP, которые можно выполнять при помощи TextBlob:\n",
        "- *разбиение на лексемы* — разбиение текста на содержательные блоки (например, слова и числа);\n",
        "- *пометка частей речи* — идентификация части речи каждого слова (существительное, глагол, прилагательное и т. д.); \n",
        "- *извлечение именных конструкций* — обнаружение групп слов, представляющих имена существительные; \n",
        "- *анализ эмоциональной окраски* — определение положительной, отрицательной или нейтральной окраски текста; \n",
        "- *перевод* на другие языки и *распознавание* языка на базе Google Translate;\n",
        "- *формообразование* — образование множественного и единственного числа. У формообразования существуют и другие аспекты, которые не поддерживаются TextBlob;\n",
        "- *проверка орфографии и исправление ошибок*;\n",
        "- *выделение основы* — исключение приставок, суффиксов и т. д.; например, при выделении основы из слова «varieties» будет получен результат «varieti»;\n",
        "- *лемматизация* — аналог выделения основы, но с формированием реальных слов на основании контекста исходных слов; например, результатом лемматизации «varieties» является слово «variety»;\n",
        "- *определение частот слов* — определение того, сколько раз каждое слово встречается в корпусе;\n",
        "- *интеграция с WordNet* для поиска определений слов, синонимов и антонимов;\n",
        "- *устранение игнорируемых слов* — исключение таких слов, как a, an, the, I, we, you и т. д., с целью анализа важных слов в корпусе;\n",
        "- *n-граммы* — построение множеств последовательно идущих слов в корпусе для выявления слов, часто располагающихся по соседству друг с другом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCYY-wOVhVca"
      },
      "source": [
        "**Токенизация** <a class=\"anchor\" id=\"fund_nltk\"></a>\n",
        "\n",
        "Токенизировать - значит, поделить текст на части: слова, ключевые слова, фразы, символы и т.д., иными словами токены.\n",
        "\n",
        "Самый наивный способ токенизировать текст - разделить с помощью функции split(). Но split упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем, поэтому лучше использовать готовые токенизаторы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5R0LNzkcZQq"
      },
      "source": [
        "import nltk # библиотека nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize # готовые токенизаторы библиотеки nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX60rn2WQ0UK",
        "outputId": "e6da57db-929f-48bd-e15f-52757c4bd992"
      },
      "source": [
        "text = 'Некоторые из более изящных функций включают добавление избыточных функций, которые в основном представляют собой линейную комбинацию существующих функций. Добавление неинформативных функций, чтобы проверить, подходит ли модель этим бесполезным функциям. Добавление непосредственно повторяющихся функций.' \n",
        "text.split(' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Некоторые',\n",
              " 'из',\n",
              " 'более',\n",
              " 'изящных',\n",
              " 'функций',\n",
              " 'включают',\n",
              " 'добавление',\n",
              " 'избыточных',\n",
              " 'функций,',\n",
              " 'которые',\n",
              " 'в',\n",
              " 'основном',\n",
              " 'представляют',\n",
              " 'собой',\n",
              " 'линейную',\n",
              " 'комбинацию',\n",
              " 'существующих',\n",
              " 'функций.',\n",
              " 'Добавление',\n",
              " 'неинформативных',\n",
              " 'функций,',\n",
              " 'чтобы',\n",
              " 'проверить,',\n",
              " 'подходит',\n",
              " 'ли',\n",
              " 'модель',\n",
              " 'этим',\n",
              " 'бесполезным',\n",
              " 'функциям.',\n",
              " 'Добавление',\n",
              " 'непосредственно',\n",
              " 'повторяющихся',\n",
              " 'функций.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_702Dg5OWg-5"
      },
      "source": [
        "В `nltk` вообще есть довольно много токенизаторов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdlvAjc7h0ng",
        "outputId": "9b12b1ae-b7ef-49c3-ad64-c368c60cb599"
      },
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordTokenizer',\n",
              " 'TweetTokenizer',\n",
              " 'WhitespaceTokenizer',\n",
              " 'WordPunctTokenizer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_VQgcLqfx0i"
      },
      "source": [
        "Чтобы использовать токенизатор ```word_tokenize```, нужно сначала скачать данные для nltk о пунктуации и стоп-словах. Это просто требование nltk, поэтому, особо не задумываясь, запустите следующую ячейку:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeZLWwwEeTLt",
        "outputId": "18d853b8-a157-42a4-ae03-03fa4f05b513"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl88F5BDdYQr",
        "outputId": "dd917f2b-6b73-4e25-b314-fc76eabf5b86"
      },
      "source": [
        "text = 'Некоторые из более изящных функций включают добавление избыточных функций, которые в основном представляют собой линейную комбинацию существующих функций. Добавление неинформативных функций, чтобы проверить, подходит ли модель этим бесполезным функциям. Добавление непосредственно повторяющихся функций.' \n",
        "sentences = word_tokenize(text)\n",
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Некоторые\n",
            "из\n",
            "более\n",
            "изящных\n",
            "функций\n",
            "включают\n",
            "добавление\n",
            "избыточных\n",
            "функций\n",
            ",\n",
            "которые\n",
            "в\n",
            "основном\n",
            "представляют\n",
            "собой\n",
            "линейную\n",
            "комбинацию\n",
            "существующих\n",
            "функций\n",
            ".\n",
            "Добавление\n",
            "неинформативных\n",
            "функций\n",
            ",\n",
            "чтобы\n",
            "проверить\n",
            ",\n",
            "подходит\n",
            "ли\n",
            "модель\n",
            "этим\n",
            "бесполезным\n",
            "функциям\n",
            ".\n",
            "Добавление\n",
            "непосредственно\n",
            "повторяющихся\n",
            "функций\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA2bii5cjHH1"
      },
      "source": [
        "**Лемматизация**\n",
        "\n",
        "Лемматизация – это сведение разных форм одного слова к начальной форме – лемме. Зачем?\n",
        "\n",
        "Во-первых, естественно рассматривать как отдельный признак каждое слово, а не каждую его отдельную форму.\n",
        "Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
        "Для русского языка есть хороший лемматизатор `pymorphy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD1uVPBtiopl",
        "outputId": "ace684fb-0dd3-4d0d-d77b-f238a952ea01"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |██████                          | 10kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 5.9MB/s \n",
            "\u001b[?25hInstalling collected packages: dawg-python, pymorphy2-dicts-ru, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4mt-nFIjm_j"
      },
      "source": [
        "В [**pymorphy2**](https://pymorphy2.readthedocs.io/en/latest/user/guide.html) для морфологического анализа слов есть MorphAnalyzer():\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vXVvTjNjf_z"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUE3bhBtj1RG",
        "outputId": "8a7b43d6-c2b2-4ebe-d850-a99af245e2eb"
      },
      "source": [
        "ana = pymorphy2_analyzer.parse('поросенка')\n",
        "ana"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='поросёнка', tag=OpencorporaTag('NOUN,anim,masc sing,gent'), normal_form='поросёнок', score=0.5, methods_stack=((DictionaryAnalyzer(), 'поросёнка', 205, 1),)),\n",
              " Parse(word='поросёнка', tag=OpencorporaTag('NOUN,anim,masc sing,accs'), normal_form='поросёнок', score=0.5, methods_stack=((DictionaryAnalyzer(), 'поросёнка', 205, 3),))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9fEiZhl8kGkh",
        "outputId": "c2f1b9a1-f285-41b6-cf25-f91540dd7adc"
      },
      "source": [
        "ana[0].normal_form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'поросёнок'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1ClSGrSlstZ"
      },
      "source": [
        "**Стоп-слова и пунктуация**\n",
        "\n",
        "**Стоп-слова** - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе. Для модели это просто шум. А шум нужно убирать. По аналогичной причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SIxxzmHlxGK",
        "outputId": "596734bc-e2dd-48fc-cfc0-dd29c5534a4b"
      },
      "source": [
        "# импортируем стоп-слова из библиотеки nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# посмотрим на стоп-слова для русского языка\n",
        "print(stopwords.words('russian'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PWhDQNFCl4Ix",
        "outputId": "fd7eea0b-dd14-4fd7-80f4-92f480805aeb"
      },
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sRIrfruoF9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18471018-b665-4c87-a6aa-f0614c5c1cfe"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |██████                          | 10kB 13.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 6.7MB/s \n",
            "\u001b[?25hInstalling collected packages: dawg-python, pymorphy2-dicts-ru, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdTc6BJ-bZb_"
      },
      "source": [
        "## Лабораторная работа: булева информационного поиска"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNVhDkmtcU1O"
      },
      "source": [
        "$\\blacktriangleright$ Формирование исходной коллекции документов "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDsx2Csjbm9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab83d2d0-27d0-4063-84f4-447e09b7bd28"
      },
      "source": [
        "import nltk # библиотека nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize # готовые токенизаторы библиотеки nltk\n",
        "from string import punctuation\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "d1 = 'Юрий Алексеевич Гагарин (9 марта 1934, Клушино, Гжатский (ныне Гагаринский) район, Западная область (ныне — Смоленская область), СССР — 27 марта 1968, возле села Новосёлово, Киржачский район, Владимирская область, СССР) — лётчик-космонавт СССР, Герой Советского Союза, кавалер высших знаков отличия ряда государств, почётный гражданин многих российских и зарубежных городов.Перейти к разделу «Почётные звания и награды»'\n",
        "d2 = 'Орёл — город (с 1566) в России, административный центр Орловской области и Орловского района, в который сам не входит. Являясь городом областного значения, образует муниципальное образование городской округ город Орёл. Расположен в 368 км к юго-западу от Москвы, на Среднерусской возвышенности в европейской части России, по обоим берегам реки Оки и её притока Орлика. Орёл и Орловская область входят в состав Центрального федерального округа, а также Центрального экономического района. Город первого салюта (5 августа 1943 года) и город воинской славы (27 апреля 2007 года).'\n",
        "d3 = 'Производная функции — понятие дифференциального исчисления, характеризующее скорость изменения функции в данной точке. Определяется как предел отношения приращения функции к приращению её аргумента при стремлении приращения аргумента к нулю, если такой предел существует. Функцию, имеющую конечную производную (в некоторой точке), называют дифференцируемой (в данной точке).'\n",
        "d4 = 'Сериал не является строго историческим и вольно интерпретирует скандинавские саги о набегах викингов на англосаксонскую Британию, Западно-Франкское королевство и другие земли. Нестрогость «Викингов» заметна по несоответствию многих дат и совмещению исторических личностей, живших в разные временные периоды. Ключевым персонажем сюжета является полулегендарный скандинавский конунг Рагнар Лодброк, который якобы вёл свой род от бога Одина. Помимо Рагнара, протагонистами также выступают его сыновья, друзья, жены Лагерта и Аслауг и другие исторические личности, например, норвежский конунг Харальд Прекрасноволосый или король Уэссекса Альфред Великий. Действие начинается с драмы одного викинга и постепенно набирает обороты, превращаясь в эпическую историю о войнах королевств. В сериале прослеживается и мистический компонент: персонажи-провидцы, духовная связь между близкими, призраки и даже воплощения богов.'\n",
        "d5 = 'Государственный строй — президентско-парламентская республика [4] с федеративным устройством. С 31 декабря 1999 года (с перерывом в 2008—2012 годах, когда Дмитрий Медведев был президентом) должность президента Российской Федерации занимает Владимир Путин. C 16 января 2020 года должность председателя Правительства РФ занимает Михаил МишустинПерейти к разделу «Государственное устройство».'\n",
        "d6 = 'Александр Александрович Мисуркин (род. 23 сентября 1977) — российский космонавт-испытатель отряда ФГБУ «НИИ ЦПК имени Ю. А. Гагарина». 116-й космонавт России (СССР) и 531-й космонавт мира. Совершил два космических полёта к Международной космической станции на транспортных пилотируемых кораблях Союз ТМА-08М и Союз МС-06. Участник основных космических экспедиций МКС-35/МКС-36, МКС-53 /МКС-54. Продолжительность полётов составила 334 суток 11 часов 29 минут. Совершил четыре выхода в открытый космос, общая продолжительность работ в открытом космосе составила 28 часов 13 минут. До поступления в отряд космонавтов служил лётчиком, командиром авиационного звена гвардейского учебно-авиационного полка Краснодарского военного авиационного института имени А. К. Серова. Подполковник ВВС (2009). Герой Российской Федерации, лётчик-космонавт Российской Федерации (2016). Почётный гражданин города Орла.'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r98QlICHdQX-"
      },
      "source": [
        "$\\blacktriangleright$ Токенизация документов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY4dovODdPeT"
      },
      "source": [
        "token_d1 = []\n",
        "token_d2 = []\n",
        "token_d3 = []\n",
        "token_d4 = []\n",
        "token_d5 = []\n",
        "token_d6 = []\n",
        "list111 = []\n",
        "\n",
        "punt_my = set(punctuation).union(set(['«','»','—']))  \n",
        "\n",
        "sentences = word_tokenize(d1)\n",
        "for sentence in sentences:\n",
        "  token_d1.append(sentence)\n",
        "list_d1 = list(set(token_d1).difference(punt_my))\n",
        "\n",
        "sentences = word_tokenize(d2)\n",
        "for sentence in sentences:\n",
        "  token_d2.append(sentence)\n",
        "list_d2 = list(set(token_d2).difference(punt_my))\n",
        "\n",
        "sentences = word_tokenize(d3)\n",
        "for sentence in sentences:\n",
        "  token_d3.append(sentence) \n",
        "list_d3 = list(set(token_d3).difference(punt_my))\n",
        "\n",
        "sentences = word_tokenize(d4)\n",
        "for sentence in sentences:\n",
        "  token_d4.append(sentence)\n",
        "list_d4 = list(set(token_d4).difference(punt_my))\n",
        "\n",
        "sentences = word_tokenize(d5)\n",
        "for sentence in sentences:\n",
        "  token_d5.append(sentence)\n",
        "list_d5 = list(set(token_d5).difference(punt_my))\n",
        "\n",
        "sentences = word_tokenize(d6)\n",
        "for sentence in sentences:\n",
        "  token_d6.append(sentence)\n",
        "list_d6 = list(set(token_d6).difference(punt_my))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnXje-ozp1Sg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_awzKATp1bi"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "list_sw1 = list(set(list_d1).difference(stopwords.words('russian')))\n",
        "list_sw2 = list(set(list_d2).difference(stopwords.words('russian')))\n",
        "list_sw3 = list(set(list_d3).difference(stopwords.words('russian')))\n",
        "list_sw4 = list(set(list_d4).difference(stopwords.words('russian')))\n",
        "list_sw5 = list(set(list_d5).difference(stopwords.words('russian')))\n",
        "list_sw6 = list(set(list_d6).difference(stopwords.words('russian')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3vMAWGKxZ5V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyNCHlDkxaHS"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFdrBs7YxaQO"
      },
      "source": [
        "#ana = pymorphy2_analyzer.parse('поросенка')\n",
        "#ana\n",
        "list_limm = []\n",
        "list_limm2 = []\n",
        "list_limm3 = []\n",
        "list_limm4 = []\n",
        "list_limm5 = []\n",
        "list_limm6 = []\n",
        "matches = []\n",
        "\n",
        "for items in list_sw1:\n",
        "  limm1 = pymorphy2_analyzer.parse(items)\n",
        "  list_limm.append(limm1[0].normal_form)\n",
        "\n",
        "for items in list_sw2:\n",
        "  limm1 = pymorphy2_analyzer.parse(items)\n",
        "  list_limm2.append(limm1[0].normal_form)\n",
        "\n",
        "for items in list_sw3:\n",
        "  limm1 = pymorphy2_analyzer.parse(items)\n",
        "  list_limm3.append(limm1[0].normal_form)\n",
        "\n",
        "for items in list_sw4:\n",
        "  limm1 = pymorphy2_analyzer.parse(items)\n",
        "  list_limm4.append(limm1[0].normal_form)\n",
        "\n",
        "for items in list_sw5:\n",
        "  limm1 = pymorphy2_analyzer.parse(items)\n",
        "  list_limm5.append(limm1[0].normal_form)\n",
        "\n",
        "for items in list_sw6:\n",
        "  limm1 = pymorphy2_analyzer.parse(items)\n",
        "  list_limm6.append(limm1[0].normal_form)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "R71bn70V4JnG",
        "outputId": "a11b9646-219a-4d7d-c59b-94c97838a72d"
      },
      "source": [
        "all_list1 = {'matches': matches,'d1': list_limm, 'd2': list_limm2, 'd3': list_limm3, 'd4': list_limm4, 'd5': list_limm5, 'd6': list_limm6}\n",
        "\n",
        "input_str = 'год'\n",
        "\n",
        "list_bul1 = []\n",
        "list_bul2 = []\n",
        "list_bul3 = []\n",
        "list_bul4 = []\n",
        "list_bul5 = []\n",
        "list_bul6 = []\n",
        "\n",
        "\n",
        "for search_1 in all_list1['d1']:\n",
        "  if search_1 == input_str:\n",
        "    list_bul1.append(1)\n",
        "  else:\n",
        "    list_bul1.append(0)    \n",
        "\n",
        "for search_1 in all_list1['d2']:\n",
        "  if search_1 == input_str:\n",
        "    list_bul2.append(1)\n",
        "  else:\n",
        "    list_bul2.append(0)\n",
        "\n",
        "for search_1 in all_list1['d3']:\n",
        "  if search_1 == input_str:\n",
        "    list_bul3.append(1)\n",
        "  else:\n",
        "    list_bul3.append(0)\n",
        "\n",
        "for search_1 in all_list1['d4']:\n",
        "  if search_1 == input_str:\n",
        "    list_bul4.append(1)\n",
        "  else:\n",
        "    list_bul4.append(0)\n",
        "\n",
        "for search_1 in all_list1['d5']:\n",
        "  if search_1 == input_str:\n",
        "    list_bul5.append(1)\n",
        "  else:\n",
        "    list_bul5.append(0)\n",
        "\n",
        "for search_1 in all_list1['d6']:\n",
        "  if search_1 == input_str:\n",
        "    list_bul6.append(1)\n",
        "  else:\n",
        "    list_bul6.append(0)\n",
        "\n",
        "list_list_1 = {'d1': list_limm, 'd2': list_limm2, 'd3': list_limm3, 'd4': list_limm4, 'd5': list_limm5, 'd6': list_limm6, 'm1': list_bul1,\n",
        "               'm2': list_bul2, 'm3': list_bul3, 'm4': list_bul4, 'm5': list_bul5, 'm6': list_bul6}\n",
        "\n",
        "df = pd.DataFrame.from_dict(list_list_1, orient='index')\n",
        "df = df.transpose()\n",
        "df.head(8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>d1</th>\n",
              "      <th>d2</th>\n",
              "      <th>d3</th>\n",
              "      <th>d4</th>\n",
              "      <th>d5</th>\n",
              "      <th>d6</th>\n",
              "      <th>m1</th>\n",
              "      <th>m2</th>\n",
              "      <th>m3</th>\n",
              "      <th>m4</th>\n",
              "      <th>m5</th>\n",
              "      <th>m6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>советский</td>\n",
              "      <td>расположить</td>\n",
              "      <td>производный</td>\n",
              "      <td>превращаться</td>\n",
              "      <td>год</td>\n",
              "      <td>город</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27</td>\n",
              "      <td>ока</td>\n",
              "      <td>исчисление</td>\n",
              "      <td>ключевой</td>\n",
              "      <td>дмитрий</td>\n",
              "      <td>выход</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ряд</td>\n",
              "      <td>год</td>\n",
              "      <td>определяться</td>\n",
              "      <td>якобы</td>\n",
              "      <td>год</td>\n",
              "      <td>общий</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>киржачский</td>\n",
              "      <td>апрель</td>\n",
              "      <td>некоторый</td>\n",
              "      <td>интерпретировать</td>\n",
              "      <td>4</td>\n",
              "      <td>цпк</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>гагарин</td>\n",
              "      <td>1566</td>\n",
              "      <td>изменение</td>\n",
              "      <td>драма</td>\n",
              "      <td>российский</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>почётный</td>\n",
              "      <td>входить</td>\n",
              "      <td>характеризовать</td>\n",
              "      <td>протагонист</td>\n",
              "      <td>устройство</td>\n",
              "      <td>час</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>государство</td>\n",
              "      <td>федеральный</td>\n",
              "      <td>нуль</td>\n",
              "      <td>сын</td>\n",
              "      <td>федерация</td>\n",
              "      <td>российский</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>юрий</td>\n",
              "      <td>её</td>\n",
              "      <td>её</td>\n",
              "      <td>викинг</td>\n",
              "      <td>31</td>\n",
              "      <td>ссср</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            d1           d2               d3                d4  ... m3 m4 m5 m6\n",
              "0    советский  расположить      производный      превращаться  ...  0  0  1  0\n",
              "1           27          ока       исчисление          ключевой  ...  0  0  0  0\n",
              "2          ряд          год     определяться             якобы  ...  0  0  1  0\n",
              "3   киржачский       апрель        некоторый  интерпретировать  ...  0  0  0  0\n",
              "4      гагарин         1566        изменение             драма  ...  0  0  0  0\n",
              "5     почётный      входить  характеризовать       протагонист  ...  0  0  0  0\n",
              "6  государство  федеральный             нуль               сын  ...  0  0  0  0\n",
              "7         юрий           её               её            викинг  ...  0  0  0  0\n",
              "\n",
              "[8 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlekDEE64JxN",
        "outputId": "056eba0a-0da9-4d52-d701-b845d23b50e6"
      },
      "source": [
        "df_copy = df.copy(deep=True)\n",
        "df_copy.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 90 entries, 0 to 89\n",
            "Data columns (total 7 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   matches  0 non-null      object\n",
            " 1   d1       42 non-null     object\n",
            " 2   d2       58 non-null     object\n",
            " 3   d3       27 non-null     object\n",
            " 4   d4       89 non-null     object\n",
            " 5   d5       37 non-null     object\n",
            " 6   d6       90 non-null     object\n",
            "dtypes: object(7)\n",
            "memory usage: 5.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81WXUoS84J68"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThAztb8v4KDN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzSD7xgm4KLV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCa1Ri5h4KTj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMCwfLGkxaSt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5yvCRHyxaVk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSX90azMxakb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avyep82NxatI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuKYrgGPxa2Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Z0Wc1Ef7ll"
      },
      "source": [
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFX0f7cbekWS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6hHryegd5p3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jtcoxlAoI_J"
      },
      "source": [
        "## Что такое n-граммы:\n",
        "\n",
        "Самые мелкие структуры языка, с которыми мы работаем, называются **n-граммами**.\n",
        "У n-граммы есть параметр n - количество слов, которые попадают в такое представление текста.\n",
        "* Если n = 1 - то мы смотрим на то, сколько раз каждое слово встретилось в тексте. Получаем _униграммы_\n",
        "* Если n = 2 - то мы смотрим на то, сколько раз каждая пара подряд идущих слов, встретилась в тексте. Получаем _биграммы_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUt8870ioTPf"
      },
      "source": [
        "Функция для работы с n-граммами реализована в библиотке **nltk**, импортируем эту функцию: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Dead59oNp6"
      },
      "source": [
        "from nltk import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypheg9HdolDW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "c669323e-a8b0-40f8-f633-19603986346d"
      },
      "source": [
        "sentence = 'Некоторые из более изящных функций включают добавление избыточных функций, которые в основном представляют собой линейную комбинацию существующих функций. Добавление неинформативных функций, чтобы проверить, подходит ли модель этим бесполезным функциям. Добавление непосредственно повторяющихся функций.'.split()\n",
        "sentence = word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6efab263737c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Некоторые из более изящных функций включают добавление избыточных функций, которые в основном представляют собой линейную комбинацию существующих функций. Добавление неинформативных функций, чтобы проверить, подходит ли модель этим бесполезным функциям. Добавление непосредственно повторяющихся функций.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt884LMCpojP"
      },
      "source": [
        "Чтобы получить n-грамму для такой последовательности, используем функцию ngrams().\n",
        "\n",
        "На вход передается два параметра:\n",
        "\n",
        "лист с разделенным на отдельные слова предложением (у нас он хранится в переменной sent);\n",
        "параметр n, определяющий, какой тип n-грамм мы хотим получить.\n",
        "Чтобы полученный объект отобразить, делаем из него list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwbCjEHEoNzK"
      },
      "source": [
        "list(ngrams(sentence, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n1ceIhuoN9b"
      },
      "source": [
        "import re\n",
        "import requests\n",
        "\n",
        "the_idiot_url = 'https://www.gutenberg.org/files/2638/2638-0.txt'\n",
        "raw = requests.get(the_idiot_url).text\n",
        "\n",
        "# Индекс начала первой главы\n",
        "start = re.search(r'\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK THE IDIOT \\*\\*\\*', raw).end()\n",
        "start\n",
        "# Индекс конца первой главы\n",
        "#end = re.search(r'II', raw).start()\n",
        "#end\n",
        "#raw[start:end].lower()\n",
        "#print('Количество отдельностоящего слова <<the>> в первой главе:', len(re.findall(r'\\Wthe\\W', raw[start:end].lower())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7Xp2H-KC6Al"
      },
      "source": [
        "# **ДОПОЛНИТЕЛЬНЫЙ МАТЕРИАЛ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhGpbglNDdiQ"
      },
      "source": [
        "Работа в текстовом режиме (markdown) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncs47bXCPcZK"
      },
      "source": [
        "<u>Шрифт</u> [ссылка](#label)\n",
        "# Заголовок 1\n",
        "## Заголовок 1.1\n",
        "- первая строка списка\n",
        "- вторая строка списка\n",
        "  - ава\n",
        "    - авав\n",
        "\n",
        "|Первый столбец|Первый столбец|\n",
        "|:---|---:|\n",
        "|первая строка|вторая строка|\n",
        "\n",
        "$\\sin(x)$\n",
        "\n",
        "$\\checkmark$ Первый пункт\n",
        "\n",
        "\n",
        "`.head()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1SL8UTOvPfM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8n_QLOpvP71"
      },
      "source": [
        "# Литература по курсу\n",
        "1. Хобсон Л. Обработка естественного языка в действии / Л. Хобсон, Х. Ханнес, Х. Коул. — СПб.: Питер, 2020. — 576 с.\n",
        "2. Николаев И.С., Митренина О.В., Ландо Т.М. Прикладная и компьютерная лингвистика / И.С. Николаев, О.В. Митренина, Т.М. Ландо. —  ЛЕНАНД, 2016. — 316 с.\n",
        "3. Васильев В.Г. Методы автоматизированной обработки текстов / В.Г. Васильев, М.П. Кривенко. — ИПИ РАН, 2008. — 304 с.  \n",
        "4. Дейтел П. Python: Искусственный интеллект, большие данные и облачные вычисления / П. Дейтел, Х. Дейтел. — СПб.: Питер, 2020. — 864 с.\n",
        "5. Элбон К. Машинное обучение с использованием Python. Сборник рецептов. — СПб.: БХВ-Петербург, 2019. — 384 с.\n",
        "6. Бенгфорт Б. Прикладной анализ текстовых данных на Python. Машинное обучение и создание приложений обработки естественного языка / Б. Бенгфорт, Р. Билбро, Т. Охеда  — СПб.: Питер, 2019. — 368 с."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZvN8tWDpXeg"
      },
      "source": [
        ""
      ]
    }
  ]
}